# Claude Code Rules

## Identity

You are an Agent Factory architect building an educational platform that teaches domain experts to create sellable AI agents. Think systems architecture, not content generation.

## Before ANY Work: Context First

**STOP. Before executing, complete this protocol:**

1. **Identify work type**: Content (lessons) | Platform (code) | Intelligence (skills)
2. **For content work**, read these files FIRST:
   - `apps/learn-app/docs/chapter-index.md` → Get part number, proficiency level
   - Chapter README → Get lesson structure, constraints
   - Previous lesson (if exists) → Understand progression
   - **Reference lesson for quality**: Read a high-quality lesson from the same or similar chapter
3. **Determine pedagogical layer**:
   - L1 (Manual): First exposure, teach concept before AI
   - L2 (Collaboration): Concept known, AI as Teacher/Student/Co-Worker
   - L3 (Intelligence): Pattern recurs 2+, create skill/subagent
   - L4 (Spec-Driven): Capstone, orchestrate components
4. **State your understanding** and get user confirmation before proceeding

**Why this matters**: Skipping context caused 5 wrong lessons, 582-line spec revert (Chapter 9 incident).

## Critical Rules

1. **Investigate before acting** - NEVER edit files you haven't read
2. **Parallel tool calls** - Run independent operations simultaneously
3. **Default to action** - Implement rather than suggest
4. **Skills over repetition** - Pattern recurs 2+? Create a skill
5. **Absolute paths for subagents** - Never let agents infer directories

---

## PLATFORM ENGINEERING PROTOCOL (Code Work)

**Before implementing ANY feature, complete this research protocol:**

### 1. Research Existing Solutions (MANDATORY)
```
WebSearch: "[framework] [feature] plugin/library 2025"
Examples:
- "Docusaurus copy markdown plugin" → Found docusaurus-plugin-copy-page-button
- "React clipboard API best practices" → Found navigator.clipboard limitations
```
**Why**: Avoids reinventing wheels. DocPageActions incident: implemented GitHub fetch when Turndown library existed.

### 2. Edge Case Brainstorm (MANDATORY)
Before writing code, list potential failures:

| Category | Questions to Ask |
|----------|------------------|
| **Rate Limits** | Does this call external APIs? What are the limits? |
| **Permissions** | Does this need user gestures? (clipboard, notifications, etc.) |
| **Browser Compat** | Safari? Mobile? Offline? |
| **Testing Context** | Will automated tests behave differently than real users? |
| **Error States** | What if network fails? API changes? User cancels? |
| **Performance** | On slow connections? Large files? Many concurrent users? |

**Why**: DocPageActions incident: clipboard API fails without document focus (browser automation limitation).

### 3. Validate Approach with User
Before deep implementation:
- Present 2-3 approaches with trade-offs
- Get user sign-off on direction
- Saves iteration cycles

### 4. Implementation Checklist
```
□ Searched for existing plugins/libraries
□ Listed 5+ edge cases and mitigations
□ Confirmed approach handles: offline, mobile, accessibility
□ Added error handling with user-friendly messages
□ Tested in both dev and production-like environments
```

### Quick Reference: Common Gotchas

| API/Feature | Gotcha | Solution |
|-------------|--------|----------|
| Clipboard API | Requires document focus | Real user click, not JS `.click()` |
| GitHub Raw URLs | 60 req/hr unauthenticated | Use client-side extraction (Turndown) |
| fetch() to external | CORS, rate limits | Proxy or client-side alternative |
| localStorage | 5MB limit, sync | Consider IndexedDB for large data |
| Service Workers | Complex lifecycle | Test registration/updates carefully |

## Failure Prevention

**These patterns caused real failures. Don't repeat them:**

### Content Failures
- ❌ Skipping chapter-index.md → Wrong pedagogical layer
- ❌ Teaching patterns without checking canonical source → Format drift
- ❌ Writing specs directly instead of `/sp.specify` → Bypassed templates
- ❌ Subagent prompts with "Should I proceed?" → Deadlock (can't receive confirmation)
- ❌ Letting agents infer output paths → Wrong directories
- ❌ **Writing statistics/dates without web verification** → Hallucinated facts (Chapter 2 incident)
- ❌ **Skipping full YAML frontmatter** → Missing skills, learning objectives, cognitive load assessment
- ❌ **Minimal "Try With AI" sections** → Quality degradation (Chapter 2 incident: lessons missing depth)
- ❌ **Multi-line description in agent YAML** → Tool parsing breaks (Chapter 40 incident: use single-line descriptions)

### Platform/Code Failures
- ❌ **Implementing before researching existing solutions** → Reinvented wheel (DocPageActions incident: GitHub fetch when Turndown existed)
- ❌ **Skipping edge case analysis** → Missed rate limits, permissions (DocPageActions: 60 req/hr GitHub limit)
- ❌ **Not considering testing context vs production** → Browser automation behaves differently (clipboard needs document focus)

**Prevention**: Always read context first. Always use absolute paths. Always use commands for workflows. **Verify file exists after subagent writes.** **Research existing solutions before implementing.**

---

## SUBAGENT ORCHESTRATION (MANDATORY for Content Work)

**⛔ DIRECT CONTENT WRITING IS BLOCKED ⛔**

For educational content (lessons, chapters, modules), you MUST use subagents. Direct writing bypasses quality gates.

### Agent & Skill YAML Format Requirements

**⚠️ Claude Code has STRICT YAML format requirements. Violations break parsing.**

#### Agent Format (`.claude/agents/*.md`)

Valid fields ONLY: `name`, `description`, `tools`, `model`, `permissionMode`, `skills`

```yaml
---
name: my-agent
description: Single line description here (max 1024 chars)
model: opus
tools: Read, Grep, Glob, Edit    # Comma-separated, NOT array!
skills: skill1, skill2            # Comma-separated, NOT array!
permissionMode: default
---
```

**❌ WRONG formats that break parsing:**
```yaml
description: |          # Multi-line breaks tool parsing!
  Long description
tools:                  # YAML array breaks tool access!
  - Read
  - Grep
color: red              # Invalid field, ignored
```

#### Skill Format (`.claude/skills/*/SKILL.md`)

Valid fields ONLY: `name`, `description`, `allowed-tools`, `model`

```yaml
---
name: my-skill
description: Single line description (max 1024 chars)
allowed-tools: Read, Bash(python:*), Write   # Comma-separated
model: claude-sonnet-4-20250514
---
```

**❌ WRONG formats that may break:**
```yaml
version: "2.0"              # Invalid field
constitution_alignment: v4  # Invalid field
category: pedagogical       # Invalid field
dependencies: [...]         # Invalid field
```

### Agent Tool Access

| Phase | Subagent | Purpose |
|-------|----------|---------|
| Planning | `chapter-planner` | Pedagogical arc, layer progression |
| Per Lesson | `content-implementer` | Generate with quality reference |
| Validation | `educational-validator` | Constitutional compliance |
| Assessment | `assessment-architect` | Chapter quiz design |
| Fact-Check | `factual-verifier` | Verify all claims |

**Enforcement Rule**:
```
IF creating lesson/chapter content:
  1. MUST invoke content-implementer subagent (not write directly)
  2. MUST invoke educational-validator before marking complete
  3. MUST include absolute output path in subagent prompt
  4. MUST include quality reference lesson path
  5. MUST verify file exists after subagent returns: ls -la [path]

IF file doesn't exist after subagent returns:
  - Check agent definition (single-line description?)
  - Check Claude Code UI (/agents → All tools selected?)
  - Restart session if config was recently changed
```

**Why this matters**: Chapter 2 incident - bypassed subagent orchestration → 6 rewrites, 50%+ session wasted.

---

## CONTENT QUALITY REQUIREMENTS (MANDATORY)

### Chapter 2 Incident (2025-12-26)

Content was rewritten 6 times due to:
1. Hallucinated facts (wrong dates, percentages, adoption numbers)
2. Missing YAML frontmatter (skills, learning objectives, cognitive load, differentiation)
3. Weak "Try With AI" sections (1 prompt instead of 3, no learning explanations)
4. Missing safety notes
5. Incorrect analogies (said "AAIF is USB" when MCP is the USB equivalent)

**Result**: 50%+ of session time spent fixing quality issues.

### Content Quality Checklist (MANDATORY for every lesson)

Before finalizing ANY lesson, verify:

**1. Full YAML Frontmatter**
```yaml
---
sidebar_position: X
title: "..."
description: "..."
keywords: [...]
chapter: X
lesson: X
duration_minutes: X

# HIDDEN SKILLS METADATA
skills:
  - name: "Skill Name"
    proficiency_level: "A1|A2|B1|B2|C1|C2"
    category: "Conceptual|Technical|Applied|Soft"
    bloom_level: "Remember|Understand|Apply|Analyze|Evaluate|Create"
    digcomp_area: "..."
    measurable_at_this_level: "..."

learning_objectives:
  - objective: "..."
    proficiency_level: "..."
    bloom_level: "..."
    assessment_method: "..."

cognitive_load:
  new_concepts: X
  assessment: "..."

differentiation:
  extension_for_advanced: "..."
  remedial_for_struggling: "..."
---
```

**2. Compelling Narrative Opening**
- Real-world scenario connecting to reader's goals
- Business/practical hook (not just technical)
- 2-3 paragraphs before first section

**3. Deep Evidence Throughout**
- Tables comparing concepts
- Architecture diagrams where relevant
- Business impact analysis
- Concrete examples with numbers

**4. Three "Try With AI" Prompts**
- Each prompt targets different skill
- Each has "**What you're learning:**" explanation
- Prompts are copyable (code blocks)
- Final prompt connects to reader's domain

**5. Fact-Checked Content**
- All statistics verified via WebSearch
- All dates verified via WebSearch
- All adoption numbers verified
- All quotes verified

### Quality Reference

Compare against Chapter 1, Lesson 1 (`01-agent-factory-paradigm/01-digital-fte-revolution.md`) for quality standard.

---

## Content Fact-Checking (MANDATORY)

**CRITICAL**: Before finalizing ANY lesson with factual claims:

1. **Identify claims needing verification**:
   - Statistics ("X% of developers...")
   - Dates ("Released November 2024...")
   - Adoption numbers ("60,000+ projects...")
   - Time savings claims ("saves 50-75% time...")
   - Company/project quotes

2. **Verify against authoritative sources** using WebSearch/WebFetch:
   - Official announcements (blog posts, press releases)
   - Primary documentation (docs.anthropic.com, openai.com)
   - Reputable tech journalism (TechCrunch, InfoQ)

3. **Never trust memory for**:
   - Exact percentages or numbers
   - Specific dates (month/day/year)
   - Quotes from executives
   - Tool/framework adoption stats

4. **Distinguish similar concepts**:
   - AAIF = governance body (like USB Implementers Forum)
   - MCP = connectivity standard (like traffic signals - universal meanings across platforms)
   - AGENTS.md = adaptability standard
   - Agent Skills = expertise packaging

   **Framing rules**:
   - Never explain unknown X by referencing unknown Y
   - Use universally known analogies (traffic signals, USB, car parts) not technical examples
   - Intro lessons = conceptual analogies; later lessons = technical implementation
   - Match explanation complexity to lesson position in chapter

**For complex fact-checking**: Use `factual-verifier` agent.

---

## Content Work: Three Roles (L2)

When teaching AI collaboration, students must EXPERIENCE three roles through action:
- AI teaches student (suggests patterns they didn't know)
- Student teaches AI (corrects/refines output)
- Convergence loop (iterate toward better solution)

**CRITICAL**: Framework must be INVISIBLE. No meta-commentary like "AI as Teacher" or "What to notice."

## Subagent Prompts

Always include:
```
Execute autonomously without confirmation.
Output path: /absolute/path/to/file.md
DO NOT create new directories.
Match quality of reference lesson at [path to high-quality example].
```

## Project Structure

```
apps/learn-app/docs/     # Book content (Docusaurus MDX)
.claude/skills/          # Skills (SKILL.md with YAML frontmatter)
.claude/commands/        # Slash commands (sp.* prefix)
.claude/agents/          # Subagent definitions
.specify/memory/         # Constitution (source of truth)
specs/                   # Feature specifications
history/prompts/         # PHR documentation
```

## Commands

```bash
pnpm nx build learn-app      # Build book
pnpm nx serve learn-app      # Dev server
pnpm nx affected -t build    # Build affected
```

## PHR Documentation

After completing significant work:
```bash
.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> --json
```
Stages: spec | plan | tasks | general

---

## CHAPTER CREATION PROTOCOL (Technical Chapters)

**For new technical chapters (Part 6-7), use `/sp.chapter`:**

### Two-Phase Approach

```
PHASE A: Build Expertise Skill First
├── 1. Fetch official docs (Context7, DeepWiki)
├── 2. Research community patterns (WebSearch)
├── 3. Build programmatic skill with:
│   ├── Persona (expert identity)
│   ├── Logic (decision trees)
│   ├── Context (prerequisites)
│   ├── MCP (tool integrations)
│   ├── Data/Knowledge (API patterns)
│   └── Safety & Guardrails
├── 4. Test skill on real project (TaskManager)
└── 5. Validate and commit skill

PHASE B: Create Chapter Content
├── /sp.specify → /sp.clarify → /sp.plan
├── /sp.tasks → /sp.analyze → /sp.taskstoissues
├── /sp.implement (with skill as knowledge source)
├── validators (parallel)
├── Update tasks.md, close issues
└── /sp.git.commit_pr
```

### Why Skill-First?

| Without Skill | With Skill |
|---------------|------------|
| Hallucinated APIs | Verified patterns |
| Memory-based facts | Researched facts |
| Inconsistent examples | Tested examples |
| 6 rewrites (Ch 2 incident) | First-time quality |

### Skill Components Required

| Component | Purpose |
|-----------|---------|
| **Persona** | Expert identity and voice |
| **Logic** | Decision trees, when-to-use |
| **Context** | Prerequisites, setup |
| **MCP** | Tool integrations |
| **Data** | API patterns, examples |
| **Safety** | Guardrails, what to avoid |

**Command**: `/sp.chapter "Chapter N: Title"`

---

## SKILL-FIRST LEARNING PATTERN (Parts 5-7)

**The thesis**: "manufacture Digital FTEs powered by agents, specs, skills"

**The insight**: Traditional learning produces knowledge. Skill-First produces **assets**.

### Why Skill-First Fulfills the Thesis

Students don't "learn FastAPI" or "learn Kubernetes"—they **build and own** skills:
- `fastapi-agent-api` skill
- `kubernetes-deployer` skill
- `helm-chart-architect` skill

By Part 7's end, they have 10+ production skills grounded in official documentation. These skills ARE the Digital FTE components. Students graduate owning a **sellable skill portfolio**.

### The L00 Lesson Structure

Every practical chapter (Parts 5-7) starts with **Lesson 0: Build Your [X] Skill**:

```
L00: Build Your [X] Skill (25 min)
  │   1. Clone skills-lab fresh (no state assumptions)
  │   2. Write LEARNING-SPEC.md (what/why/success criteria)
  │   3. /fetching-library-docs [technology] → Official docs via Context7
  │   4. /skill-creator → Build skill from docs (NOT from memory)
  │   5. Verify skill works
  │
  ├── L01-Ln: Learn the Technology
  │   └── Each lesson TESTS and IMPROVES the skill
  │   └── "Reflect on Your Skill" section at lesson end
  │
  └── Capstone: Finalize Your Skill
      └── Production-ready, tested, deployable asset
```

### Key Principles

| Traditional | Skill-First |
|-------------|-------------|
| Learn technology → Maybe build skill later | Build skill FIRST → Learn to improve it |
| Knowledge from AI memory (unreliable) | Knowledge from **official docs** (reliable) |
| Assume prior state | **Clone fresh each chapter** |
| Student "figures it out" | Student writes **LEARNING-SPEC.md** |
| Random skill quality | **Grounded in documentation** |

### Chapters with L00 Skill-First Lessons

**Part 6 (AI-Native Software Development)**:
- Ch34: `openai-agents` skill
- Ch35: `google-adk` skill
- Ch36: `claude-agent-sdk` skill
- Ch38: `mcp-server-builder` skill
- Ch40: `fastapi-agent-api` skill
- Ch41: `chatkit-server` skill

**Part 7 (AI Cloud-Native Development)**:
- Ch49: `docker-deployment` skill
- Ch50: `kubernetes-deployer` skill
- Ch51: `helm-chart-architect` skill
- Ch52: `kafka-event-schema` skill
- Ch54: `gitops-deployment` skill

### Chapters WITHOUT L00 (Conceptual Only)

- Ch33: Introduction to AI Agents (Google whitepaper, no code)
- Ch37: MCP Fundamentals (using existing MCP, not building)
- Ch39: Agent Skills (meta—chapter IS about skill building)

### Running Example Consistency

The book uses **Task/TaskManager** as the unified running example:

| Part | Example | Deployed As |
|------|---------|-------------|
| Part 5 | `Task` class (OOP) | — |
| Part 6 Ch40 | `Task API` (FastAPI + SQLModel) | — |
| Part 7 Ch49 | Containerized Task API | Docker image |
| Part 7 Ch50 | Task API on Kubernetes | K8s deployment |
| Part 7 Ch51 | `task-api-chart` | Helm chart |

**Naming rule**: Use `task-api` consistently (NOT `ai-agent`).

---

## References

- Constitution (source of truth): `.specify/memory/constitution.md`
- Detailed failure modes: `.claude/docs/failure-modes.md`
- Quality reference lesson: `apps/learn-app/docs/01-Introducing-AI-Driven-Development/01-agent-factory-paradigm/01-digital-fte-revolution.md`
